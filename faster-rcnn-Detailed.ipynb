{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T07:10:27.453083Z",
     "iopub.status.busy": "2024-02-29T07:10:27.452735Z",
     "iopub.status.idle": "2024-02-29T07:10:27.465614Z",
     "shell.execute_reply": "2024-02-29T07:10:27.464596Z",
     "shell.execute_reply.started": "2024-02-29T07:10:27.453054Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn \n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image, ImageDraw\n",
    "import requests \n",
    "from pathlib import Path \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import cv2 \n",
    "import matplotlib.pyplot as plt \n",
    "from pathlib import Path\n",
    "import random,os\n",
    "from skimage import io\n",
    "from pycocotools.coco import COCO\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as mpatches\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "import cv2\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "    x1A, y1A, x2A, y2A = boxA  # Coordinates of boxA (x1, y1, x2, y2)\n",
    "    # Convert boxB from (x1, y1, width, height) to (x1, y1, x2, y2)\n",
    "    x1B, y1B = boxB[0], boxB[1]\n",
    "    x2B, y2B = boxB[0] + boxB[2], boxB[1] + boxB[3]\n",
    "    # Calculate intersection coordinates\n",
    "    xA = max(x1A, x1B)\n",
    "    yA = max(y1A, y1B)\n",
    "    xB = min(x2A, x2B)\n",
    "    yB = min(y2A, y2B)\n",
    "    # Calculate intersection area\n",
    "    intersection_area = max(0, xB - xA) * max(0, yB - yA)\n",
    "    # Calculate areas of both boxes\n",
    "    boxAArea = (x2A - x1A) * (y2A - y1A)\n",
    "    boxBArea = (x2B - x1B) * (y2B - y1B)\n",
    "    # Calculate union area\n",
    "    union_area = boxAArea + boxBArea - intersection_area\n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / float(union_area + 1e-6)  # Adding epsilon to avoid division by zero\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T07:10:27.467089Z",
     "iopub.status.busy": "2024-02-29T07:10:27.466813Z",
     "iopub.status.idle": "2024-02-29T07:10:28.598620Z",
     "shell.execute_reply": "2024-02-29T07:10:28.597638Z",
     "shell.execute_reply.started": "2024-02-29T07:10:27.467055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.63s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = \"Dataset\"\n",
    "annFile = Path('Models/instances_val2017.json')\n",
    "coco = COCO(annFile)\n",
    "imgIds = coco.getImgIds() # load all validation set ids \n",
    "annIds = coco.getAnnIds()\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco_names = [\"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"airplane\" , \"bus\" , \"train\" , \"truck\" , \"boat\" , \"traffic light\" , \"fire hydrant\" , \"street sign\" , \"stop sign\" , \"parking meter\" , \"bench\" , \"bird\" , \"cat\" , \"dog\" , \"horse\" , \"sheep\" , \"cow\" , \"elephant\" , \"bear\" , \"zebra\" , \"giraffe\" , \"hat\" , \"backpack\" , \"umbrella\" , \"shoe\" , \"eye glasses\" , \"handbag\" , \"tie\" , \"suitcase\" , \n",
    "\"frisbee\" , \"skis\" , \"snowboard\" , \"sports ball\" , \"kite\" , \"baseball bat\" , \n",
    "\"baseball glove\" , \"skateboard\" , \"surfboard\" , \"tennis racket\" , \"bottle\" , \n",
    "\"plate\" , \"wine glass\" , \"cup\" , \"fork\" , \"knife\" , \"spoon\" , \"bowl\" , \n",
    "\"banana\" , \"apple\" , \"sandwich\" , \"orange\" , \"broccoli\" , \"carrot\" , \"hot dog\" ,\n",
    "\"pizza\" , \"donut\" , \"cake\" , \"chair\" , \"couch\" , \"potted plant\" , \"bed\" ,\n",
    "\"mirror\" , \"dining table\" , \"window\" , \"desk\" , \"toilet\" , \"door\" , \"tv\" ,\n",
    "\"laptop\" , \"mouse\" , \"remote\" , \"keyboard\" , \"cell phone\" , \"microwave\" ,\n",
    "\"oven\" , \"toaster\" , \"sink\" , \"refrigerator\" , \"blender\" , \"book\" ,\n",
    "\"clock\" , \"vase\" , \"scissors\" , \"teddy bear\" , \"hair drier\" , \"toothbrush\" , \"hair brush\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T07:10:28.600041Z",
     "iopub.status.busy": "2024-02-29T07:10:28.599754Z",
     "iopub.status.idle": "2024-02-29T07:10:30.324447Z",
     "shell.execute_reply": "2024-02-29T07:10:30.323439Z",
     "shell.execute_reply.started": "2024-02-29T07:10:28.600016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.84s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\SEM FOLDERS\\V SEM FOLDERS\\hackathon\\p1env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\SEM FOLDERS\\V SEM FOLDERS\\hackathon\\p1env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms \n",
    "transform = transforms.ToTensor()\n",
    "coco_dataset = CocoDetection(root=DATA_PATH, annFile=annFile, transform=transform)\n",
    "data_loader = DataLoader(coco_dataset, batch_size=1, shuffle=False)\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T07:10:30.327265Z",
     "iopub.status.busy": "2024-02-29T07:10:30.326965Z",
     "iopub.status.idle": "2024-02-29T07:10:30.821171Z",
     "shell.execute_reply": "2024-02-29T07:10:30.820203Z",
     "shell.execute_reply.started": "2024-02-29T07:10:30.327239Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Dataset\\\\000000000139.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m coco\u001b[38;5;241m.\u001b[39mloadAnns(ann_ids)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Visualize the first image with ground truth bounding boxes and labels \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Load the image using PIL \u001b[39;00m\n\u001b[0;32m     12\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m ax\u001b[38;5;241m.\u001b[39mimshow(img)\n",
      "File \u001b[1;32mD:\\SEM FOLDERS\\V SEM FOLDERS\\hackathon\\p1env\\lib\\site-packages\\PIL\\Image.py:3092\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3089\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3092\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3093\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Dataset\\\\000000000139.jpg'"
     ]
    }
   ],
   "source": [
    "# Load the first image using COCO API \n",
    "first_image_id = coco_dataset.ids[0]  # Get the ID of the first image in the dataset \n",
    "image_data = coco.loadImgs(first_image_id)[0]\n",
    "image_path = os.path.join(DATA_PATH, image_data['file_name'])\n",
    "\n",
    "# Get the ground truth annotations for the first image \n",
    "ann_ids = coco.getAnnIds(imgIds=first_image_id)\n",
    "ground_truth = coco.loadAnns(ann_ids)\n",
    "\n",
    "# Visualize the first image with ground truth bounding boxes and labels \n",
    "img = Image.open(image_path).convert(\"RGB\")  # Load the image using PIL \n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(img)\n",
    "\n",
    "# Plot ground truth bounding boxes with labels for the first image \n",
    "for annotation in ground_truth:\n",
    "    bbox = annotation['bbox']\n",
    "    label = coco.loadCats(annotation['category_id'])[0]['name']  # Get category label \n",
    "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3],\n",
    "                             linewidth=2, edgecolor='b', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(bbox[0], bbox[1], label, color='b', fontsize=10, va='top')  # Display label near the box\n",
    "\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T07:10:30.822758Z",
     "iopub.status.busy": "2024-02-29T07:10:30.822448Z",
     "iopub.status.idle": "2024-02-29T07:10:40.405505Z",
     "shell.execute_reply": "2024-02-29T07:10:40.404475Z",
     "shell.execute_reply.started": "2024-02-29T07:10:30.822732Z"
    }
   },
   "outputs": [],
   "source": [
    "score_threshold = 0.75\n",
    "iou_scores_total = []\n",
    "low_iou_indices = []\n",
    "counter = 0  # Counter to limit computation to the first 3 samples\n",
    "for images, targets in tqdm(data_loader):\n",
    "    if counter >= 3:  # Break the loop after processing 3 samples\n",
    "        break\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = list(image for image in images)\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        for pred, target in zip(predictions, targets):\n",
    "            counter += 1\n",
    "            if counter > 3:\n",
    "                break\n",
    "            \n",
    "            # Extract predicted boxes and scores above threshold\n",
    "            pred_boxes = pred['boxes'].detach().numpy()\n",
    "            scores = pred['scores'].detach().numpy()\n",
    "            # Filter boxes based on score threshold\n",
    "            high_score_mask = scores > score_threshold\n",
    "            pred_boxes = pred_boxes[high_score_mask]\n",
    "\n",
    "            # Get ground truth annotations for the image\n",
    "            image_id = target['image_id'].item()\n",
    "            ann_ids = coco.getAnnIds(imgIds=image_id)\n",
    "            ground_truth = coco.loadAnns(ann_ids)\n",
    "            ground_truth_boxes = [anno['bbox'] for anno in ground_truth]\n",
    "\n",
    "            # Calculate IoU for each ground truth box with predicted boxes\n",
    "            iou_scores = []\n",
    "            for pred_box in pred_boxes:\n",
    "                ious = [calculate_iou(pred_box, gt_box) for gt_box in ground_truth_boxes]\n",
    "                iou_scores.append(max(ious))  # Take the highest IoU per ground truth box\n",
    "            \n",
    "            # Calculate mean IoU for the entire sample\n",
    "            iou_mean = sum(iou_scores) / len(iou_scores)\n",
    "            iou_scores_total.append(iou_mean)\n",
    "            if iou_mean < 0.2:\n",
    "                low_iou_indices.append(counter - 1)\n",
    "            # Print IoU for the current sample\n",
    "            print(f\"IoU for Sample {counter}: {iou_mean:.4f}\")\n",
    "# Calculate the mean IoU for the processed samples\n",
    "mean_iou_model = sum(iou_scores_total) / len(iou_scores_total)\n",
    "print(f\"Mean IoU of the model (first 3 samples): {mean_iou_model:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-29T07:10:40.407306Z",
     "iopub.status.busy": "2024-02-29T07:10:40.406988Z",
     "iopub.status.idle": "2024-02-29T07:10:54.212151Z",
     "shell.execute_reply": "2024-02-29T07:10:54.211248Z",
     "shell.execute_reply.started": "2024-02-29T07:10:40.407261Z"
    }
   },
   "outputs": [],
   "source": [
    "score_threshold = 0.75\n",
    "counter = 0 \n",
    "for images, targets in tqdm(data_loader):\n",
    "    if counter >= 4:\n",
    "        break\n",
    "    with torch.no_grad():\n",
    "        images = list(image for image in images)\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        for img, pred, target in zip(images, predictions, targets):\n",
    "            counter += 1\n",
    "            if counter > 4:\n",
    "                break\n",
    "            # Convert tensor image to PIL for visualization\n",
    "            img_pil = to_pil_image(img)\n",
    "            # Extract predicted boxes and scores above threshold\n",
    "            pred_boxes = pred['boxes'].detach().numpy()\n",
    "            scores = pred['scores'].detach().numpy()\n",
    "            # Filter boxes based on score threshold\n",
    "            high_score_mask = scores > score_threshold\n",
    "            pred_boxes = pred_boxes[high_score_mask]\n",
    "            # Get ground truth annotations for the image\n",
    "            image_id = target['image_id'].item()\n",
    "            ann_ids = coco.getAnnIds(imgIds=image_id)\n",
    "            ground_truth = coco.loadAnns(ann_ids)\n",
    "            ground_truth_boxes = [anno['bbox'] for anno in ground_truth]\n",
    "            # Calculate IoU for each predicted box with ground truth boxes\n",
    "            iou_scores = []\n",
    "            for pred_box in pred_boxes:\n",
    "                ious = [calculate_iou(pred_box, gt_box) for gt_box in ground_truth_boxes]\n",
    "                iou_scores.append(max(ious))  # Take the highest IoU\n",
    "            # Display image with predicted boxes and IoU information\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.imshow(img_pil)\n",
    "            ax = plt.gca()\n",
    "            for box, iou in zip(pred_boxes, iou_scores):\n",
    "                box = list(box)\n",
    "                rect = plt.Rectangle((box[0], box[1]), box[2] - box[0], box[3] - box[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "                plt.text(box[0], box[1], f\"IoU: {iou:.2f}\", bbox=dict(facecolor='red', alpha=0.5))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
